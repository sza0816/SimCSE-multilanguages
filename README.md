# Notes for Progress Report — Understanding SimCSE Pipeline (English)

> ⚠️ This is **not** a project README and **not** a guide for running code**.  
> It is only for helping group members understand the workflow so they can write the progress report.

We trained **unsupervised** and **supervised** English SimCSE models on a Vast.ai GPU instance (A5000 / RTX 4090).  
Below is a clear explanation of the workflow, code structure, and evaluation results.

---

## 1. Why Vast.ai?

We experimented with several compute environments:

- **SeaWulf cluster** → GPU nodes too busy, pending too long  
- **Google Cloud** → complicated setup, no free GPUs available  
- **Brother’s RTX5060 PC** → too new; PyTorch currently supports up to sm90, not sm120  
- **Vast.ai** → easiest, cheapest, instant GPU rental

For the report:

> We used Vast.ai GPU instances (A5000/RTX4090) to train and evaluate our SimCSE models.

---

## 2. Repository Structure (Important for understanding the pipeline)

```
SimCSE-multilanguages/
│
├── bash/
│   ├── train_english.sh        # wrapper for training (unsup / sup)
│   └── eval_english.sh         # wrapper for STS‑B evaluation
│
├── data/
│   └── download_scripts/
│       ├── download_unsup.sh
│       ├── download_sup.sh
│       └── download_sts.sh
│
├── data_collator/
│   ├── data_collator_unsup.py
│   └── data_collator_sup.py
│
├── outputs/
│   └── en/
│       ├── unsup/
│       │   ├── logs/
│       │   └── eval/
│       └── sup/
│           ├── logs/
│           └── eval/
│
├── simcse_model.py
├── train_english.py
├── evaluate_english.py
├── setup.sh
└── requirements.txt
```

Report phrasing:

> The pipeline consists of setup scripts, a unified training module, supervised/unsupervised data collators, and an STS‑B evaluation module.

---

## 3. Workflow Overview (What the report should describe)

## 3.0 SimCSE Training Pipeline (Diagram)

```mermaid
flowchart LR
A["Input Dataset"] --> B["Data Collator (unsup: dropout pairs / sup: NLI triplets)"]
B --> C["SimCSE Model (BERT Encoder + CLS pooling)"]
C --> D["Compute similarity matrix (cosine or dot product)"]
D --> E["Contrastive Objective (InfoNCE)"]
E --> F["Backprop + Optimizer"]
```

The entire experiment contains **three steps**:

---

### STEP 1 — Environment Setup

Run:

```
bash bash/setup.sh
```

The script:

- prints Python / pip / CUDA versions
- installs required packages (transformers, datasets, accelerate, torch…)
- downloads datasets:
  - `wiki_english.txt` (unsupervised)
  - `nli_english.csv` (supervised)
  - STS‑B dev/test/train tsv files

Report line:

> setup.sh installs all dependencies and downloads SimCSE datasets automatically.

---

### STEP 2 — Training (unsupervised / supervised)

Training is done using:

```
bash bash/train_english.sh
```

The script sets:

- `MODE` (unsup or sup)
- hyperparameters (both modes use the same ones for now)
- log output path
- checkpoint path

Then it calls:

```
train_english.py
```

### What happens inside `train_english.py`?

| Mode | Dataset | Data collator | Objective |
|------|---------|----------------|-----------|
| `unsup` | wiki | dropout‑based positive pairs | contrastive |
| `sup` | NLI triples | anchor/positive/hard‑negative | supervised contrastive |

Training logs are saved to:

```
outputs/en/<mode>/logs/train.log
```

Checkpoints are saved to:

```
outputs/en/<mode>/checkpoints/
```

Report line:

> We trained both unsupervised and supervised SimCSE models using the same script, with different data collators for unsupervised vs. supervised contrastive learning.

---

### STEP 3 — STS‑B Evaluation

Evaluation script:

```
bash bash/eval_english.sh
```

This calls `evaluate_english.py`, which:

- loads the trained checkpoint  
- embeds each sentence pair  
- computes cosine similarity  
- computes **Spearman correlation** between similarity and gold labels  
- writes summary to:

```
outputs/en/<mode>/eval/summary.txt
```

Note: STS‑B **test** has no gold labels → always `nan`.

---


## 3.1. Contrastive Learning Objective (InfoNCE Loss)

Our implementation follows the SimCSE paper and uses the **InfoNCE contrastive loss**, even though the name “InfoNCE” does not explicitly appear in the code.

**Why InfoNCE?**  
SimCSE frames sentence embedding learning as a contrastive problem:
- pull **positive pairs** closer  
- push **negative samples** apart  

### Unsupervised SimCSE  
Positive pairs are generated by applying **two different dropout masks** to the same sentence.  
In‑batch samples serve as negatives.  
The loss has the form:

![InfoNCE Loss](https://latex.codecogs.com/png.image?\dpi{150}-\log\frac{\exp(\text{sim}(z_i,z_i%5E%2B)/\tau)}{\sum_j\exp(\text{sim}(z_i,z_j)/\tau)})

This is exactly the **InfoNCE / NT‑Xent** loss used in SimCLR and adopted by SimCSE.

### Supervised SimCSE  
Each NLI triplet provides:
- anchor (sent0)
- positive (entailment)
- hard negative (contradiction)

The same InfoNCE structure is applied, treating the positive as the only correct target among all negatives (including in‑batch negatives).

### Why this matters for our report  
Even if the code does not mention the term “InfoNCE”,  
**our training objective is mathematically identical to the contrastive loss described in the SimCSE paper**,  
so our reproduction is faithful to the original method.

## 4. Final English Results

### **Unsupervised SimCSE**
```
STS‑dev:   0.6854
STS‑test:  nan
STS‑train: 0.6253
```

### **Supervised SimCSE**
```
STS‑dev:   0.7315
STS‑test:  nan
STS‑train: 0.6296
```

Report writing suggestion:

> The supervised model achieved higher STS‑B dev correlation (0.73) compared to the unsupervised model (0.68), consistent with the findings of the original SimCSE paper.

---

## 5. Summary for Progress Report

Below is a summary paragraph that group members can take as reference:

> We used Vast.ai GPU instances (A5000/RTX4090) to train English SimCSE models.  
> The environment and datasets were prepared using setup.sh, which installs dependencies and downloads wiki (unsupervised), NLI triples (supervised), and STS‑B benchmarks.  
> Our unified training script (train_english.py) supports both unsupervised and supervised modes via different data collators.  
> After training, we evaluated sentence embeddings on STS‑B using Spearman correlation between cosine similarity and human‑annotated similarity scores.  
> The unsupervised model achieved 0.6854 on STS‑dev, while the supervised model reached 0.7315, showing clear improvement from supervised contrastive learning.

---
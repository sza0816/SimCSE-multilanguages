#!/usr/bin/env python3
"""
This is the unified training script for SimCSE, supporting both unsupervised and supervised modes,
and multiple languages including English, Chinese, and Hindi.
The pipeline involves loading the appropriate dataset, tokenizing, and training the SimCSE model.
Unsupervised mode leverages dropout to create positive pairs, while supervised mode uses NLI triplets.
The model internally uses the InfoNCE loss for contrastive learning.
"""
import argparse
import os
import torch
from transformers import AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset

import random
import numpy as np
from datasets.utils.logging import disable_progress_bar
disable_progress_bar()

# Enforce reproducibility by fixing all random seeds and disabling nondeterministic CuDNN settings.
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

from simcse_model import SimCSEModel
from data_collator.data_collator_unsup import DataCollatorForSimCSE
from data_collator.data_collator_sup import DataCollatorForSupervisedSimCSE


def parse_args():
    parser = argparse.ArgumentParser()

    # Required
    parser.add_argument("--mode", type=str, required=True,
                        choices=["unsup", "sup"],
                        help="Choose unsupervised or supervised SimCSE.")

    parser.add_argument("--data_path", type=str, required=True)
    parser.add_argument("--output_dir", type=str, required=True)

    # Optional
    parser.add_argument("--model_name", type=str, default="bert-base-uncased")
    parser.add_argument("--epochs", type=float, default=1.0)
    parser.add_argument("--batch_size", type=int, default=64)
    parser.add_argument("--lr", type=float, default=5e-5)
    parser.add_argument("--max_len", type=int, default=32)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--warmup_ratio", type=float, default=0.0)
    parser.add_argument("--lang", type=str, default="en", choices=["en", "ch", "hi"])

    return parser.parse_args()


# Unsupervised SimCSE uses wiki1m sentences; positive pairs are generated by dropout inside the data collator.
def load_unsup_dataset(path, tokenizer, max_len):
    dataset = load_dataset("text", data_files=path)["train"]
    dataset = dataset.map(lambda x: {"sentence": x["text"]}, num_proc=4)
    dataset = dataset.map(
        lambda e: tokenizer(e["sentence"], truncation=True, max_length=max_len),
        batched=True,
        num_proc=4,
    )
    return dataset


# Supervised SimCSE uses NLI triplets (anchor, positive, hard negative), each tokenized separately,
# and flattened into Dataset-friendly dicts.
def load_sup_dataset(path, tokenizer, max_len):
    dataset = load_dataset("csv", data_files=path)["train"]

    def tokenize_batch(batch):
        # tokenize anchor
        anchor = tokenizer(
            batch["sent0"],
            truncation=True,
            max_length=max_len,
        )
        # tokenize positive
        positive = tokenizer(
            batch["sent1"],
            truncation=True,
            max_length=max_len,
        )
        # tokenize hard negative
        negative = tokenizer(
            batch["hard_neg"],
            truncation=True,
            max_length=max_len,
        )

        # These will be consumed by the supervised data collator to construct InfoNCE triplets.
        return {
            # anchor
            "anchor_input_ids": anchor["input_ids"],
            "anchor_attention_mask": anchor["attention_mask"],

            # positive
            "positive_input_ids": positive["input_ids"],
            "positive_attention_mask": positive["attention_mask"],

            # negative
            "negative_input_ids": negative["input_ids"],
            "negative_attention_mask": negative["attention_mask"],
        }

    dataset = dataset.map(tokenize_batch, batched=True)
    return dataset


def main():
    args = parse_args()
    set_seed(args.seed)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    model = SimCSEModel(args.model_name)

    os.makedirs(args.output_dir, exist_ok=True)

    # Load dataset + collator
    if args.mode == "unsup":
        dataset = load_unsup_dataset(args.data_path, tokenizer, args.max_len)
        collator = DataCollatorForSimCSE(tokenizer)
    else:
        dataset = load_sup_dataset(args.data_path, tokenizer, args.max_len)
        collator = DataCollatorForSupervisedSimCSE(tokenizer)

    # HuggingFace TrainingArguments specify training hyperparameters and settings,
    # including FP16 training, batch size, and removing unused columns for efficiency.
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        logging_steps=200,
        save_strategy="no",
        remove_unused_columns=False,
        fp16=True,
        warmup_ratio=args.warmup_ratio,
        optim="adamw_torch",
        seed=args.seed,
        data_seed=args.seed,
        disable_tqdm=True,
        logging_dir=os.path.join(args.output_dir, "logs"),
        logging_strategy="steps",
        log_level="info",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=collator,
    )

    trainer.train()  # HuggingFace Trainer handles the full training loop; model internally computes InfoNCE contrastive loss.

    # Manual saving ensures compatibility with Vast/Seawulf and outputs all necessary files:
    # pytorch_model.bin, config.txt, and tokenizer files.
    torch.save(model.state_dict(), os.path.join(args.output_dir, "pytorch_model.bin"))
    with open(os.path.join(args.output_dir, "config.txt"), "w") as f:
        f.write(args.model_name)
    tokenizer.save_pretrained(args.output_dir)


if __name__ == "__main__":
    main()
/workspace/SimCSE-multilanguages/train_english.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 109,482,240
{'loss': 1.1449, 'grad_norm': 1.1558786630630493, 'learning_rate': 4.9376e-05, 'epoch': 0.0128}
{'loss': 1.1101, 'grad_norm': 1.3672523498535156, 'learning_rate': 4.8736e-05, 'epoch': 0.0256}
{'loss': 1.1102, 'grad_norm': 1.1113367080688477, 'learning_rate': 4.8096e-05, 'epoch': 0.0384}
{'loss': 1.1098, 'grad_norm': 1.1067243814468384, 'learning_rate': 4.7456e-05, 'epoch': 0.0512}
{'loss': 1.1082, 'grad_norm': 0.9805994629859924, 'learning_rate': 4.681600000000001e-05, 'epoch': 0.064}
{'loss': 1.1076, 'grad_norm': 0.9797989726066589, 'learning_rate': 4.6176e-05, 'epoch': 0.0768}
{'loss': 1.1083, 'grad_norm': 0.9941462874412537, 'learning_rate': 4.5536e-05, 'epoch': 0.0896}
{'loss': 1.1077, 'grad_norm': 1.030934453010559, 'learning_rate': 4.4896000000000005e-05, 'epoch': 0.1024}
{'loss': 1.107, 'grad_norm': 1.0052731037139893, 'learning_rate': 4.4256000000000005e-05, 'epoch': 0.1152}
{'loss': 1.1062, 'grad_norm': 0.9879280924797058, 'learning_rate': 4.3616000000000004e-05, 'epoch': 0.128}
{'loss': 1.1061, 'grad_norm': 0.8583061099052429, 'learning_rate': 4.2975999999999996e-05, 'epoch': 0.1408}

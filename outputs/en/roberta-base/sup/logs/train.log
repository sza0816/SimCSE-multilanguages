Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 275,601
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 4,307
  Number of trainable parameters = 124,645,632
{'loss': 0.2061, 'grad_norm': 0.02260158397257328, 'learning_rate': 4.77246343162294e-05, 'epoch': 0.046436034362665427}
{'loss': 0.0004, 'grad_norm': 0.011440404690802097, 'learning_rate': 4.540283259809612e-05, 'epoch': 0.09287206872533085}
{'loss': 0.0005, 'grad_norm': 0.039243292063474655, 'learning_rate': 4.3081030879962856e-05, 'epoch': 0.1393081030879963}
{'loss': 0.0003, 'grad_norm': 0.004906075540930033, 'learning_rate': 4.075922916182958e-05, 'epoch': 0.1857441374506617}
{'loss': 0.0003, 'grad_norm': 0.00663053710013628, 'learning_rate': 3.843742744369631e-05, 'epoch': 0.23218017181332715}
{'loss': 0.0002, 'grad_norm': 0.013316091150045395, 'learning_rate': 3.611562572556304e-05, 'epoch': 0.2786162061759926}
{'loss': 0.0002, 'grad_norm': 0.009902775287628174, 'learning_rate': 3.379382400742977e-05, 'epoch': 0.325052240538658}
{'loss': 0.0002, 'grad_norm': 0.015576844103634357, 'learning_rate': 3.147202228929649e-05, 'epoch': 0.3714882749013234}
{'loss': 0.0005, 'grad_norm': 0.005953981541097164, 'learning_rate': 2.9150220571163223e-05, 'epoch': 0.4179243092639889}
{'loss': 0.0001, 'grad_norm': 0.04315818101167679, 'learning_rate': 2.6828418853029952e-05, 'epoch': 0.4643603436266543}
{'loss': 0.0002, 'grad_norm': 0.005157884210348129, 'learning_rate': 2.450661713489668e-05, 'epoch': 0.5107963779893198}
{'loss': 0.0002, 'grad_norm': 0.0026001243386417627, 'learning_rate': 2.218481541676341e-05, 'epoch': 0.5572324123519852}
{'loss': 0.0001, 'grad_norm': 0.0019889629911631346, 'learning_rate': 1.9863013698630137e-05, 'epoch': 0.6036684467146506}
{'loss': 0.0001, 'grad_norm': 0.03319523110985756, 'learning_rate': 1.7541211980496867e-05, 'epoch': 0.650104481077316}
{'loss': 0.0001, 'grad_norm': 0.004140621051192284, 'learning_rate': 1.5219410262363596e-05, 'epoch': 0.6965405154399814}
{'loss': 0.0001, 'grad_norm': 0.006570006720721722, 'learning_rate': 1.2897608544230324e-05, 'epoch': 0.7429765498026468}
{'loss': 0.0001, 'grad_norm': 0.009344779886305332, 'learning_rate': 1.0575806826097052e-05, 'epoch': 0.7894125841653122}
{'loss': 0.0001, 'grad_norm': 0.009301544167101383, 'learning_rate': 8.25400510796378e-06, 'epoch': 0.8358486185279778}
{'loss': 0.0001, 'grad_norm': 0.0020826745312660933, 'learning_rate': 5.932203389830509e-06, 'epoch': 0.8822846528906432}
{'loss': 0.0001, 'grad_norm': 0.0024807462468743324, 'learning_rate': 3.6104016716972373e-06, 'epoch': 0.9287206872533086}
{'loss': 0.0001, 'grad_norm': 0.0025794680695980787, 'learning_rate': 1.2885999535639657e-06, 'epoch': 0.975156721615974}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 853.1059, 'train_samples_per_second': 323.056, 'train_steps_per_second': 5.049, 'train_loss': 0.009758409749935349, 'epoch': 1.0}
tokenizer config file saved in ./outputs/en/roberta-base/sup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/en/roberta-base/sup/checkpoints/special_tokens_map.json

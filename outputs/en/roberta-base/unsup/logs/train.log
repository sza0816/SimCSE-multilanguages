Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/workspace/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 124,645,632
{'loss': 1.334, 'grad_norm': 2.8510451316833496, 'learning_rate': 4.9376e-05, 'epoch': 0.0128}
{'loss': 1.1258, 'grad_norm': 2.989851713180542, 'learning_rate': 4.8736e-05, 'epoch': 0.0256}
{'loss': 1.1228, 'grad_norm': 2.728734016418457, 'learning_rate': 4.8096e-05, 'epoch': 0.0384}
{'loss': 1.1212, 'grad_norm': 2.4136407375335693, 'learning_rate': 4.7456e-05, 'epoch': 0.0512}
{'loss': 1.1196, 'grad_norm': 2.231464147567749, 'learning_rate': 4.681600000000001e-05, 'epoch': 0.064}
{'loss': 1.1169, 'grad_norm': 2.2646336555480957, 'learning_rate': 4.6176e-05, 'epoch': 0.0768}
{'loss': 1.1183, 'grad_norm': 2.2264795303344727, 'learning_rate': 4.5536e-05, 'epoch': 0.0896}
{'loss': 1.119, 'grad_norm': 2.0993306636810303, 'learning_rate': 4.4896000000000005e-05, 'epoch': 0.1024}
{'loss': 1.1161, 'grad_norm': 2.2381982803344727, 'learning_rate': 4.4256000000000005e-05, 'epoch': 0.1152}
{'loss': 1.1156, 'grad_norm': 1.9038938283920288, 'learning_rate': 4.3616000000000004e-05, 'epoch': 0.128}
{'loss': 1.1151, 'grad_norm': 1.8130444288253784, 'learning_rate': 4.2975999999999996e-05, 'epoch': 0.1408}
{'loss': 1.1143, 'grad_norm': 1.826812982559204, 'learning_rate': 4.2336e-05, 'epoch': 0.1536}
{'loss': 1.114, 'grad_norm': 1.8189427852630615, 'learning_rate': 4.1696e-05, 'epoch': 0.1664}
{'loss': 1.1138, 'grad_norm': 1.8455804586410522, 'learning_rate': 4.1056e-05, 'epoch': 0.1792}
{'loss': 1.1119, 'grad_norm': 1.8811767101287842, 'learning_rate': 4.0416e-05, 'epoch': 0.192}
{'loss': 1.1129, 'grad_norm': 1.9060698747634888, 'learning_rate': 3.9776e-05, 'epoch': 0.2048}
{'loss': 1.1126, 'grad_norm': 1.6601319313049316, 'learning_rate': 3.9136e-05, 'epoch': 0.2176}
{'loss': 1.1122, 'grad_norm': 1.770638346672058, 'learning_rate': 3.8496000000000005e-05, 'epoch': 0.2304}
{'loss': 1.1121, 'grad_norm': 1.694068431854248, 'learning_rate': 3.7856000000000005e-05, 'epoch': 0.2432}
{'loss': 1.1105, 'grad_norm': 1.544663429260254, 'learning_rate': 3.7216000000000004e-05, 'epoch': 0.256}
{'loss': 1.1123, 'grad_norm': 1.500770092010498, 'learning_rate': 3.6576e-05, 'epoch': 0.2688}
{'loss': 1.1116, 'grad_norm': 1.460400938987732, 'learning_rate': 3.5936e-05, 'epoch': 0.2816}
{'loss': 1.1103, 'grad_norm': 1.4933851957321167, 'learning_rate': 3.5296e-05, 'epoch': 0.2944}
{'loss': 1.1106, 'grad_norm': 1.488159418106079, 'learning_rate': 3.4656e-05, 'epoch': 0.3072}
{'loss': 1.1105, 'grad_norm': 1.364680290222168, 'learning_rate': 3.4016e-05, 'epoch': 0.32}
{'loss': 1.1104, 'grad_norm': 1.4486573934555054, 'learning_rate': 3.3376e-05, 'epoch': 0.3328}
{'loss': 1.1102, 'grad_norm': 1.455649971961975, 'learning_rate': 3.2736e-05, 'epoch': 0.3456}
{'loss': 1.1089, 'grad_norm': 1.3612116575241089, 'learning_rate': 3.2096000000000006e-05, 'epoch': 0.3584}
{'loss': 1.1095, 'grad_norm': 1.3980311155319214, 'learning_rate': 3.1456000000000005e-05, 'epoch': 0.3712}
{'loss': 1.1087, 'grad_norm': 1.4470826387405396, 'learning_rate': 3.0816e-05, 'epoch': 0.384}
{'loss': 1.1094, 'grad_norm': 1.2766896486282349, 'learning_rate': 3.0175999999999997e-05, 'epoch': 0.3968}
{'loss': 1.1102, 'grad_norm': 1.4581453800201416, 'learning_rate': 2.9536000000000003e-05, 'epoch': 0.4096}
{'loss': 1.1086, 'grad_norm': 3.037121534347534, 'learning_rate': 2.8896000000000003e-05, 'epoch': 0.4224}
{'loss': 1.1092, 'grad_norm': 3.585369825363159, 'learning_rate': 2.8256e-05, 'epoch': 0.4352}
{'loss': 1.11, 'grad_norm': 1.5843383073806763, 'learning_rate': 2.7616000000000005e-05, 'epoch': 0.448}
{'loss': 1.1094, 'grad_norm': 1.4590753316879272, 'learning_rate': 2.6976e-05, 'epoch': 0.4608}
{'loss': 1.1092, 'grad_norm': 1.2914679050445557, 'learning_rate': 2.6336e-05, 'epoch': 0.4736}
{'loss': 1.1084, 'grad_norm': 1.2540761232376099, 'learning_rate': 2.5696000000000003e-05, 'epoch': 0.4864}
{'loss': 1.1076, 'grad_norm': 1.6795581579208374, 'learning_rate': 2.5056000000000002e-05, 'epoch': 0.4992}
{'loss': 1.1077, 'grad_norm': 1.2092152833938599, 'learning_rate': 2.4416e-05, 'epoch': 0.512}
{'loss': 1.1077, 'grad_norm': 1.1934106349945068, 'learning_rate': 2.3776e-05, 'epoch': 0.5248}
{'loss': 1.1088, 'grad_norm': 1.2566266059875488, 'learning_rate': 2.3136000000000003e-05, 'epoch': 0.5376}
{'loss': 1.107, 'grad_norm': 1.2427375316619873, 'learning_rate': 2.2496e-05, 'epoch': 0.5504}
{'loss': 1.1074, 'grad_norm': 1.1854939460754395, 'learning_rate': 2.1859200000000002e-05, 'epoch': 0.5632}
{'loss': 1.1066, 'grad_norm': 1.216854453086853, 'learning_rate': 2.1219200000000002e-05, 'epoch': 0.576}
{'loss': 1.1072, 'grad_norm': 1.1720620393753052, 'learning_rate': 2.05792e-05, 'epoch': 0.5888}
{'loss': 1.1065, 'grad_norm': 1.2382489442825317, 'learning_rate': 1.9939200000000004e-05, 'epoch': 0.6016}
{'loss': 1.107, 'grad_norm': 1.1190742254257202, 'learning_rate': 1.92992e-05, 'epoch': 0.6144}
{'loss': 1.107, 'grad_norm': 1.1416188478469849, 'learning_rate': 1.8659200000000003e-05, 'epoch': 0.6272}
{'loss': 1.106, 'grad_norm': 1.1047000885009766, 'learning_rate': 1.80192e-05, 'epoch': 0.64}
{'loss': 1.106, 'grad_norm': 1.0672109127044678, 'learning_rate': 1.73792e-05, 'epoch': 0.6528}
{'loss': 1.106, 'grad_norm': 1.0525107383728027, 'learning_rate': 1.67392e-05, 'epoch': 0.6656}
{'loss': 1.1054, 'grad_norm': 1.3573377132415771, 'learning_rate': 1.60992e-05, 'epoch': 0.6784}
{'loss': 1.1065, 'grad_norm': 1.2319648265838623, 'learning_rate': 1.5459200000000003e-05, 'epoch': 0.6912}
{'loss': 1.1053, 'grad_norm': 1.1439378261566162, 'learning_rate': 1.48192e-05, 'epoch': 0.704}
{'loss': 1.1055, 'grad_norm': 1.0904994010925293, 'learning_rate': 1.4179200000000001e-05, 'epoch': 0.7168}
{'loss': 1.1064, 'grad_norm': 0.9637099504470825, 'learning_rate': 1.35424e-05, 'epoch': 0.7296}
{'loss': 1.1062, 'grad_norm': 1.419681429862976, 'learning_rate': 1.29024e-05, 'epoch': 0.7424}
{'loss': 1.1048, 'grad_norm': 1.0087202787399292, 'learning_rate': 1.22624e-05, 'epoch': 0.7552}
{'loss': 1.1048, 'grad_norm': 1.0747742652893066, 'learning_rate': 1.16224e-05, 'epoch': 0.768}
{'loss': 1.1051, 'grad_norm': 1.0268535614013672, 'learning_rate': 1.0982400000000002e-05, 'epoch': 0.7808}
{'loss': 1.1056, 'grad_norm': 1.1716994047164917, 'learning_rate': 1.0342400000000001e-05, 'epoch': 0.7936}
{'loss': 1.1044, 'grad_norm': 1.5784533023834229, 'learning_rate': 9.7024e-06, 'epoch': 0.8064}
{'loss': 1.1058, 'grad_norm': 0.9671139717102051, 'learning_rate': 9.0624e-06, 'epoch': 0.8192}
{'loss': 1.1047, 'grad_norm': 1.018949031829834, 'learning_rate': 8.4224e-06, 'epoch': 0.832}
{'loss': 1.1037, 'grad_norm': 1.0191173553466797, 'learning_rate': 7.7824e-06, 'epoch': 0.8448}
{'loss': 1.1045, 'grad_norm': 0.9433676600456238, 'learning_rate': 7.1424000000000004e-06, 'epoch': 0.8576}
{'loss': 1.1048, 'grad_norm': 0.9290865659713745, 'learning_rate': 6.5024e-06, 'epoch': 0.8704}
{'loss': 1.1043, 'grad_norm': 1.053378939628601, 'learning_rate': 5.862400000000001e-06, 'epoch': 0.8832}
{'loss': 1.105, 'grad_norm': 0.8956384062767029, 'learning_rate': 5.2224e-06, 'epoch': 0.896}
{'loss': 1.1047, 'grad_norm': 0.9583342671394348, 'learning_rate': 4.585600000000001e-06, 'epoch': 0.9088}
{'loss': 1.1035, 'grad_norm': 0.9847102761268616, 'learning_rate': 3.9456e-06, 'epoch': 0.9216}
{'loss': 1.1045, 'grad_norm': 0.9697635173797607, 'learning_rate': 3.3056e-06, 'epoch': 0.9344}
{'loss': 1.1043, 'grad_norm': 0.9714109897613525, 'learning_rate': 2.6656e-06, 'epoch': 0.9472}
{'loss': 1.104, 'grad_norm': 1.249100923538208, 'learning_rate': 2.0256e-06, 'epoch': 0.96}
{'loss': 1.1037, 'grad_norm': 0.9718507528305054, 'learning_rate': 1.3856000000000001e-06, 'epoch': 0.9728}
{'loss': 1.1046, 'grad_norm': 0.9635938405990601, 'learning_rate': 7.456e-07, 'epoch': 0.9856}
{'loss': 1.1046, 'grad_norm': 0.9890085458755493, 'learning_rate': 1.0560000000000002e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 3443.4469, 'train_samples_per_second': 290.407, 'train_steps_per_second': 4.538, 'train_loss': 1.1120547010498048, 'epoch': 1.0}
tokenizer config file saved in ./outputs/en/roberta-base/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/en/roberta-base/unsup/checkpoints/special_tokens_map.json

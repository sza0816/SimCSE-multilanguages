===== GPU INFO =====
Tue Nov 18 02:02:35 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA RTX A5000               On  |   00000000:81:00.0 Off |                  Off |
| 30%   31C    P8             19W /  230W |       2MiB /  24564MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
===== TRAINING CONFIG =====
MODE           = sup
LANG           = en
MODEL_NAME     = bert-base-uncased
DATA_PATH      = ./data/english/sup/nli_for_simcse.csv
OUTPUT_DIR     = ./outputs/en/sup/checkpoints
BATCH_SIZE     = 64
EPOCHS         = 1
LR             = 5e-5
MAX_LEN        = 32
/workspace/SimCSE-multilanguages/train_english.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 275,601
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 4,307
  Number of trainable parameters = 109,482,240
{'loss': 0.0618, 'grad_norm': 0.025067733600735664, 'learning_rate': 4.773624332482006e-05, 'epoch': 0.046436034362665427}
{'loss': 0.0003, 'grad_norm': 0.004703609272837639, 'learning_rate': 4.5414441606686795e-05, 'epoch': 0.09287206872533085}
{'loss': 0.0003, 'grad_norm': 0.014572548680007458, 'learning_rate': 4.309263988855352e-05, 'epoch': 0.1393081030879963}
{'loss': 0.0002, 'grad_norm': 0.007272997871041298, 'learning_rate': 4.077083817042025e-05, 'epoch': 0.1857441374506617}
{'loss': 0.0002, 'grad_norm': 0.00771720614284277, 'learning_rate': 3.8449036452286973e-05, 'epoch': 0.23218017181332715}
{'loss': 0.0001, 'grad_norm': 0.0029211463406682014, 'learning_rate': 3.6127234734153706e-05, 'epoch': 0.2786162061759926}
{'loss': 0.0001, 'grad_norm': 0.0018817061791196465, 'learning_rate': 3.380543301602043e-05, 'epoch': 0.325052240538658}
{'loss': 0.0001, 'grad_norm': 0.0014808698324486613, 'learning_rate': 3.148363129788716e-05, 'epoch': 0.3714882749013234}
{'loss': 0.0002, 'grad_norm': 0.0020093917846679688, 'learning_rate': 2.916182957975389e-05, 'epoch': 0.4179243092639889}
{'loss': 0.0001, 'grad_norm': 0.0014030688907951117, 'learning_rate': 2.684002786162062e-05, 'epoch': 0.4643603436266543}
{'loss': 0.0001, 'grad_norm': 0.0015336667420342565, 'learning_rate': 2.4518226143487347e-05, 'epoch': 0.5107963779893198}
{'loss': 0.0001, 'grad_norm': 0.002207556739449501, 'learning_rate': 2.2196424425354076e-05, 'epoch': 0.5572324123519852}
{'loss': 0.0002, 'grad_norm': 0.0012824353761970997, 'learning_rate': 1.9874622707220806e-05, 'epoch': 0.6036684467146506}
{'loss': 0.0001, 'grad_norm': 0.006209358107298613, 'learning_rate': 1.7552820989087532e-05, 'epoch': 0.650104481077316}
{'loss': 0.0001, 'grad_norm': 0.03189849108457565, 'learning_rate': 1.5231019270954262e-05, 'epoch': 0.6965405154399814}
{'loss': 0.0001, 'grad_norm': 0.008596770465373993, 'learning_rate': 1.290921755282099e-05, 'epoch': 0.7429765498026468}
{'loss': 0.0001, 'grad_norm': 0.003202874446287751, 'learning_rate': 1.0587415834687719e-05, 'epoch': 0.7894125841653122}
{'loss': 0.0001, 'grad_norm': 0.0021729571744799614, 'learning_rate': 8.265614116554447e-06, 'epoch': 0.8358486185279778}
{'loss': 0.0001, 'grad_norm': 0.0009998362511396408, 'learning_rate': 5.943812398421175e-06, 'epoch': 0.8822846528906432}
{'loss': 0.0001, 'grad_norm': 0.0010453267022967339, 'learning_rate': 3.622010680287904e-06, 'epoch': 0.9287206872533086}
{'loss': 0.0001, 'grad_norm': 0.0006940994644537568, 'learning_rate': 1.300208962154632e-06, 'epoch': 0.975156721615974}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 407.7674, 'train_samples_per_second': 675.878, 'train_steps_per_second': 10.562, 'train_loss': 0.003008438345838, 'epoch': 1.0}
tokenizer config file saved in ./outputs/en/sup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/en/sup/checkpoints/special_tokens_map.json
===== TRAINING DONE =====
Logs saved to: ./outputs/en/sup/logs/train.log
Checkpoints saved to: ./outputs/en/sup/checkpoints
Total training time: 441 seconds

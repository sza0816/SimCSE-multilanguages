/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 102,267,648
{'loss': 1.1243, 'grad_norm': 1.7117222547531128, 'learning_rate': 4.93728e-05, 'epoch': 0.0128}
{'loss': 1.1115, 'grad_norm': 1.5506937503814697, 'learning_rate': 4.87328e-05, 'epoch': 0.0256}
{'loss': 1.1114, 'grad_norm': 1.474711537361145, 'learning_rate': 4.80928e-05, 'epoch': 0.0384}
{'loss': 1.1096, 'grad_norm': 1.5417234897613525, 'learning_rate': 4.74528e-05, 'epoch': 0.0512}
{'loss': 1.1098, 'grad_norm': 1.2658833265304565, 'learning_rate': 4.68128e-05, 'epoch': 0.064}
{'loss': 1.1099, 'grad_norm': 1.3062247037887573, 'learning_rate': 4.6172800000000006e-05, 'epoch': 0.0768}
{'loss': 1.1091, 'grad_norm': 1.277098536491394, 'learning_rate': 4.5532800000000006e-05, 'epoch': 0.0896}
{'loss': 1.109, 'grad_norm': 1.30800461769104, 'learning_rate': 4.48928e-05, 'epoch': 0.1024}
{'loss': 1.1087, 'grad_norm': 1.2790236473083496, 'learning_rate': 4.42528e-05, 'epoch': 0.1152}
{'loss': 1.1098, 'grad_norm': 1.143383502960205, 'learning_rate': 4.3612800000000004e-05, 'epoch': 0.128}
{'loss': 1.108, 'grad_norm': 1.2426849603652954, 'learning_rate': 4.29728e-05, 'epoch': 0.1408}
{'loss': 1.1083, 'grad_norm': 1.3191145658493042, 'learning_rate': 4.23328e-05, 'epoch': 0.1536}
{'loss': 1.1083, 'grad_norm': 1.1066473722457886, 'learning_rate': 4.16928e-05, 'epoch': 0.1664}
{'loss': 1.1081, 'grad_norm': 1.1273077726364136, 'learning_rate': 4.10528e-05, 'epoch': 0.1792}
{'loss': 1.1077, 'grad_norm': 1.1090892553329468, 'learning_rate': 4.04128e-05, 'epoch': 0.192}
{'loss': 1.1076, 'grad_norm': 1.1110178232192993, 'learning_rate': 3.9772800000000007e-05, 'epoch': 0.2048}
{'loss': 1.1077, 'grad_norm': 1.3082869052886963, 'learning_rate': 3.9139200000000006e-05, 'epoch': 0.2176}
{'loss': 1.1077, 'grad_norm': 1.112318992614746, 'learning_rate': 3.84992e-05, 'epoch': 0.2304}
{'loss': 1.1073, 'grad_norm': 1.074256181716919, 'learning_rate': 3.78592e-05, 'epoch': 0.2432}
{'loss': 1.1083, 'grad_norm': 0.9972142577171326, 'learning_rate': 3.7219200000000004e-05, 'epoch': 0.256}
{'loss': 1.1062, 'grad_norm': 0.9783567190170288, 'learning_rate': 3.6579200000000004e-05, 'epoch': 0.2688}
{'loss': 1.1071, 'grad_norm': 1.027740716934204, 'learning_rate': 3.59392e-05, 'epoch': 0.2816}
{'loss': 1.1068, 'grad_norm': 0.9619681239128113, 'learning_rate': 3.5299199999999996e-05, 'epoch': 0.2944}
{'loss': 1.1065, 'grad_norm': 0.9227795600891113, 'learning_rate': 3.46592e-05, 'epoch': 0.3072}
{'loss': 1.1057, 'grad_norm': 0.9913387298583984, 'learning_rate': 3.40192e-05, 'epoch': 0.32}
{'loss': 1.1066, 'grad_norm': 0.9930809736251831, 'learning_rate': 3.33792e-05, 'epoch': 0.3328}
{'loss': 1.1066, 'grad_norm': 0.9523415565490723, 'learning_rate': 3.27392e-05, 'epoch': 0.3456}
{'loss': 1.1056, 'grad_norm': 0.9240822792053223, 'learning_rate': 3.20992e-05, 'epoch': 0.3584}
{'loss': 1.1053, 'grad_norm': 0.9110857248306274, 'learning_rate': 3.14592e-05, 'epoch': 0.3712}
{'loss': 1.1055, 'grad_norm': 0.9437622427940369, 'learning_rate': 3.0819200000000005e-05, 'epoch': 0.384}
{'loss': 1.1048, 'grad_norm': 0.9927675127983093, 'learning_rate': 3.01792e-05, 'epoch': 0.3968}
{'loss': 1.1052, 'grad_norm': 0.9370877742767334, 'learning_rate': 2.95392e-05, 'epoch': 0.4096}
{'loss': 1.1056, 'grad_norm': 0.8528792858123779, 'learning_rate': 2.8899200000000003e-05, 'epoch': 0.4224}
{'loss': 1.1048, 'grad_norm': 0.8368151783943176, 'learning_rate': 2.8259200000000002e-05, 'epoch': 0.4352}
{'loss': 1.1048, 'grad_norm': 0.8602834939956665, 'learning_rate': 2.76192e-05, 'epoch': 0.448}
{'loss': 1.1049, 'grad_norm': 0.8797794580459595, 'learning_rate': 2.6979199999999997e-05, 'epoch': 0.4608}
{'loss': 1.1043, 'grad_norm': 0.8603977560997009, 'learning_rate': 2.6339200000000004e-05, 'epoch': 0.4736}
{'loss': 1.1047, 'grad_norm': 0.8480829000473022, 'learning_rate': 2.56992e-05, 'epoch': 0.4864}
{'loss': 1.1042, 'grad_norm': 0.8149257302284241, 'learning_rate': 2.50592e-05, 'epoch': 0.4992}
{'loss': 1.1046, 'grad_norm': 0.8230869174003601, 'learning_rate': 2.44192e-05, 'epoch': 0.512}
{'loss': 1.1048, 'grad_norm': 0.8700764179229736, 'learning_rate': 2.37792e-05, 'epoch': 0.5248}
{'loss': 1.1045, 'grad_norm': 0.8080874681472778, 'learning_rate': 2.31392e-05, 'epoch': 0.5376}
{'loss': 1.1044, 'grad_norm': 0.8375800848007202, 'learning_rate': 2.24992e-05, 'epoch': 0.5504}
{'loss': 1.1046, 'grad_norm': 0.8394774794578552, 'learning_rate': 2.1859200000000002e-05, 'epoch': 0.5632}
{'loss': 1.1035, 'grad_norm': 0.8701622486114502, 'learning_rate': 2.1219200000000002e-05, 'epoch': 0.576}
{'loss': 1.1044, 'grad_norm': 0.7887123227119446, 'learning_rate': 2.05792e-05, 'epoch': 0.5888}
{'loss': 1.1042, 'grad_norm': 0.8379644155502319, 'learning_rate': 1.9939200000000004e-05, 'epoch': 0.6016}
{'loss': 1.1042, 'grad_norm': 0.8237982392311096, 'learning_rate': 1.92992e-05, 'epoch': 0.6144}
{'loss': 1.1041, 'grad_norm': 0.7995730042457581, 'learning_rate': 1.8659200000000003e-05, 'epoch': 0.6272}
{'loss': 1.1035, 'grad_norm': 0.8732567429542542, 'learning_rate': 1.80192e-05, 'epoch': 0.64}
{'loss': 1.1036, 'grad_norm': 0.8310962319374084, 'learning_rate': 1.73792e-05, 'epoch': 0.6528}
{'loss': 1.1043, 'grad_norm': 0.870456874370575, 'learning_rate': 1.67392e-05, 'epoch': 0.6656}
{'loss': 1.1036, 'grad_norm': 0.8060791492462158, 'learning_rate': 1.60992e-05, 'epoch': 0.6784}
{'loss': 1.1039, 'grad_norm': 0.7641057372093201, 'learning_rate': 1.5459200000000003e-05, 'epoch': 0.6912}
{'loss': 1.1042, 'grad_norm': 0.7194245457649231, 'learning_rate': 1.48192e-05, 'epoch': 0.704}
{'loss': 1.1038, 'grad_norm': 0.6871324181556702, 'learning_rate': 1.4179200000000001e-05, 'epoch': 0.7168}
{'loss': 1.1034, 'grad_norm': 0.7292677164077759, 'learning_rate': 1.3539200000000002e-05, 'epoch': 0.7296}
{'loss': 1.1036, 'grad_norm': 0.7205738425254822, 'learning_rate': 1.28992e-05, 'epoch': 0.7424}
{'loss': 1.1029, 'grad_norm': 0.759870171546936, 'learning_rate': 1.2259200000000001e-05, 'epoch': 0.7552}
{'loss': 1.1032, 'grad_norm': 0.7465150356292725, 'learning_rate': 1.16192e-05, 'epoch': 0.768}
{'loss': 1.1032, 'grad_norm': 0.7436326742172241, 'learning_rate': 1.09792e-05, 'epoch': 0.7808}
{'loss': 1.1032, 'grad_norm': 0.7472396492958069, 'learning_rate': 1.0339200000000001e-05, 'epoch': 0.7936}
{'loss': 1.1036, 'grad_norm': 0.7705678343772888, 'learning_rate': 9.6992e-06, 'epoch': 0.8064}
{'loss': 1.1028, 'grad_norm': 0.6881765127182007, 'learning_rate': 9.059200000000001e-06, 'epoch': 0.8192}
{'loss': 1.1031, 'grad_norm': 0.7108222842216492, 'learning_rate': 8.4192e-06, 'epoch': 0.832}
{'loss': 1.1032, 'grad_norm': 0.6908519268035889, 'learning_rate': 7.7792e-06, 'epoch': 0.8448}
{'loss': 1.1026, 'grad_norm': 0.6810256242752075, 'learning_rate': 7.1392e-06, 'epoch': 0.8576}
{'loss': 1.1025, 'grad_norm': 0.7028180956840515, 'learning_rate': 6.4991999999999995e-06, 'epoch': 0.8704}
{'loss': 1.1032, 'grad_norm': 0.7070626616477966, 'learning_rate': 5.8592e-06, 'epoch': 0.8832}
{'loss': 1.1028, 'grad_norm': 0.7068509459495544, 'learning_rate': 5.219200000000001e-06, 'epoch': 0.896}
{'loss': 1.1022, 'grad_norm': 0.7061255574226379, 'learning_rate': 4.5792e-06, 'epoch': 0.9088}
{'loss': 1.1023, 'grad_norm': 0.7140939831733704, 'learning_rate': 3.9392e-06, 'epoch': 0.9216}
{'loss': 1.1022, 'grad_norm': 0.7092673778533936, 'learning_rate': 3.2992e-06, 'epoch': 0.9344}
{'loss': 1.1019, 'grad_norm': 0.6764047741889954, 'learning_rate': 2.6592000000000003e-06, 'epoch': 0.9472}
{'loss': 1.1026, 'grad_norm': 0.7607424259185791, 'learning_rate': 2.0192e-06, 'epoch': 0.96}
{'loss': 1.1027, 'grad_norm': 0.6892979741096497, 'learning_rate': 1.3792e-06, 'epoch': 0.9728}
{'loss': 1.1019, 'grad_norm': 0.6678677201271057, 'learning_rate': 7.424000000000001e-07, 'epoch': 0.9856}
{'loss': 1.1025, 'grad_norm': 0.6866437792778015, 'learning_rate': 1.024e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2298.4079, 'train_samples_per_second': 435.084, 'train_steps_per_second': 6.798, 'train_loss': 1.1054996363525391, 'epoch': 1.0}
tokenizer config file saved in ./outputs/ch/bert-base-chinese/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/ch/bert-base-chinese/unsup/checkpoints/special_tokens_map.json

/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 102,267,648
{'loss': 1.1566, 'grad_norm': 1.7980908155441284, 'learning_rate': 4.93696e-05, 'epoch': 0.0128}
{'loss': 1.1222, 'grad_norm': 1.474764108657837, 'learning_rate': 4.87296e-05, 'epoch': 0.0256}
{'loss': 1.1212, 'grad_norm': 1.3909168243408203, 'learning_rate': 4.80896e-05, 'epoch': 0.0384}
{'loss': 1.1184, 'grad_norm': 1.2493259906768799, 'learning_rate': 4.74496e-05, 'epoch': 0.0512}
{'loss': 1.1173, 'grad_norm': 1.1560258865356445, 'learning_rate': 4.680960000000001e-05, 'epoch': 0.064}
{'loss': 1.1168, 'grad_norm': 1.145808458328247, 'learning_rate': 4.61696e-05, 'epoch': 0.0768}
{'loss': 1.1139, 'grad_norm': 0.9610379338264465, 'learning_rate': 4.55296e-05, 'epoch': 0.0896}
{'loss': 1.1128, 'grad_norm': 1.0058066844940186, 'learning_rate': 4.4889600000000005e-05, 'epoch': 0.1024}
{'loss': 1.1118, 'grad_norm': 0.9166212677955627, 'learning_rate': 4.4249600000000004e-05, 'epoch': 0.1152}
{'loss': 1.1119, 'grad_norm': 0.9914579391479492, 'learning_rate': 4.3609600000000003e-05, 'epoch': 0.128}
{'loss': 1.1107, 'grad_norm': 0.9774964451789856, 'learning_rate': 4.29728e-05, 'epoch': 0.1408}
{'loss': 1.1104, 'grad_norm': 1.1561967134475708, 'learning_rate': 4.23328e-05, 'epoch': 0.1536}
{'loss': 1.1104, 'grad_norm': 0.8944717049598694, 'learning_rate': 4.16928e-05, 'epoch': 0.1664}
{'loss': 1.1103, 'grad_norm': 0.8613218665122986, 'learning_rate': 4.10528e-05, 'epoch': 0.1792}
{'loss': 1.1091, 'grad_norm': 0.8901343941688538, 'learning_rate': 4.04128e-05, 'epoch': 0.192}
{'loss': 1.1094, 'grad_norm': 0.913473904132843, 'learning_rate': 3.9772800000000007e-05, 'epoch': 0.2048}
{'loss': 1.1094, 'grad_norm': 0.9185692071914673, 'learning_rate': 3.9132800000000006e-05, 'epoch': 0.2176}
{'loss': 1.1076, 'grad_norm': 0.9350823163986206, 'learning_rate': 3.84928e-05, 'epoch': 0.2304}
{'loss': 1.1098, 'grad_norm': 0.8289961218833923, 'learning_rate': 3.78528e-05, 'epoch': 0.2432}
{'loss': 1.1128, 'grad_norm': 0.7953434586524963, 'learning_rate': 3.7216000000000004e-05, 'epoch': 0.256}
{'loss': 1.1074, 'grad_norm': 0.8140384554862976, 'learning_rate': 3.6576e-05, 'epoch': 0.2688}
{'loss': 1.1082, 'grad_norm': 0.9064615368843079, 'learning_rate': 3.5936e-05, 'epoch': 0.2816}
{'loss': 1.1081, 'grad_norm': 0.8031711578369141, 'learning_rate': 3.5296e-05, 'epoch': 0.2944}
{'loss': 1.1075, 'grad_norm': 0.8481977581977844, 'learning_rate': 3.4656e-05, 'epoch': 0.3072}
{'loss': 1.1084, 'grad_norm': 0.7822516560554504, 'learning_rate': 3.4016e-05, 'epoch': 0.32}
{'loss': 1.1071, 'grad_norm': 0.8472433090209961, 'learning_rate': 3.3376e-05, 'epoch': 0.3328}
{'loss': 1.1073, 'grad_norm': 0.761962354183197, 'learning_rate': 3.2736e-05, 'epoch': 0.3456}
{'loss': 1.1071, 'grad_norm': 0.7336786985397339, 'learning_rate': 3.2096000000000006e-05, 'epoch': 0.3584}
{'loss': 1.1076, 'grad_norm': 0.7672399878501892, 'learning_rate': 3.1456000000000005e-05, 'epoch': 0.3712}
{'loss': 1.1065, 'grad_norm': 0.7474018335342407, 'learning_rate': 3.0816e-05, 'epoch': 0.384}
{'loss': 1.1065, 'grad_norm': 0.7460043430328369, 'learning_rate': 3.0175999999999997e-05, 'epoch': 0.3968}
{'loss': 1.1073, 'grad_norm': 0.8477563261985779, 'learning_rate': 2.9536000000000003e-05, 'epoch': 0.4096}
{'loss': 1.1064, 'grad_norm': 0.7154932618141174, 'learning_rate': 2.8896000000000003e-05, 'epoch': 0.4224}
{'loss': 1.1072, 'grad_norm': 0.7056550979614258, 'learning_rate': 2.8256e-05, 'epoch': 0.4352}
{'loss': 1.106, 'grad_norm': 0.7294232249259949, 'learning_rate': 2.7616000000000005e-05, 'epoch': 0.448}
{'loss': 1.1064, 'grad_norm': 0.8001274466514587, 'learning_rate': 2.6976e-05, 'epoch': 0.4608}
{'loss': 1.1065, 'grad_norm': 1.1541789770126343, 'learning_rate': 2.6336e-05, 'epoch': 0.4736}
{'loss': 1.1061, 'grad_norm': 0.7093352675437927, 'learning_rate': 2.5696000000000003e-05, 'epoch': 0.4864}
{'loss': 1.1067, 'grad_norm': 0.6684008836746216, 'learning_rate': 2.5056000000000002e-05, 'epoch': 0.4992}
{'loss': 1.1114, 'grad_norm': 1.3564890623092651, 'learning_rate': 2.4422400000000002e-05, 'epoch': 0.512}
{'loss': 1.1062, 'grad_norm': 0.7091627717018127, 'learning_rate': 2.37824e-05, 'epoch': 0.5248}
{'loss': 1.1065, 'grad_norm': 0.694089412689209, 'learning_rate': 2.31424e-05, 'epoch': 0.5376}
{'loss': 1.1055, 'grad_norm': 0.6683588624000549, 'learning_rate': 2.25024e-05, 'epoch': 0.5504}
{'loss': 1.1053, 'grad_norm': 0.6755815744400024, 'learning_rate': 2.1862400000000003e-05, 'epoch': 0.5632}
{'loss': 1.1056, 'grad_norm': 0.7035146951675415, 'learning_rate': 2.1222400000000002e-05, 'epoch': 0.576}
{'loss': 1.106, 'grad_norm': 0.641571581363678, 'learning_rate': 2.05824e-05, 'epoch': 0.5888}
{'loss': 1.1054, 'grad_norm': 0.6472854018211365, 'learning_rate': 1.99424e-05, 'epoch': 0.6016}
{'loss': 1.106, 'grad_norm': 0.6642512083053589, 'learning_rate': 1.93024e-05, 'epoch': 0.6144}
{'loss': 1.1052, 'grad_norm': 0.6510506868362427, 'learning_rate': 1.8662400000000003e-05, 'epoch': 0.6272}
{'loss': 1.1052, 'grad_norm': 0.6751649975776672, 'learning_rate': 1.80224e-05, 'epoch': 0.64}
{'loss': 1.1055, 'grad_norm': 0.611932635307312, 'learning_rate': 1.73824e-05, 'epoch': 0.6528}
{'loss': 1.1049, 'grad_norm': 0.6532160043716431, 'learning_rate': 1.67424e-05, 'epoch': 0.6656}
{'loss': 1.1051, 'grad_norm': 0.6099112033843994, 'learning_rate': 1.61024e-05, 'epoch': 0.6784}
{'loss': 1.1049, 'grad_norm': 0.6208375096321106, 'learning_rate': 1.5462400000000003e-05, 'epoch': 0.6912}
{'loss': 1.1046, 'grad_norm': 0.6221916675567627, 'learning_rate': 1.48224e-05, 'epoch': 0.704}
{'loss': 1.1051, 'grad_norm': 0.6821144819259644, 'learning_rate': 1.4182400000000002e-05, 'epoch': 0.7168}
{'loss': 1.1047, 'grad_norm': 0.6811655163764954, 'learning_rate': 1.35424e-05, 'epoch': 0.7296}
{'loss': 1.1052, 'grad_norm': 0.6084426641464233, 'learning_rate': 1.29024e-05, 'epoch': 0.7424}
{'loss': 1.1049, 'grad_norm': 0.7335746884346008, 'learning_rate': 1.22624e-05, 'epoch': 0.7552}
{'loss': 1.1042, 'grad_norm': 0.6615836024284363, 'learning_rate': 1.16224e-05, 'epoch': 0.768}
{'loss': 1.1042, 'grad_norm': 0.6309549808502197, 'learning_rate': 1.0982400000000002e-05, 'epoch': 0.7808}
{'loss': 1.1043, 'grad_norm': 0.6025271415710449, 'learning_rate': 1.0342400000000001e-05, 'epoch': 0.7936}
{'loss': 1.1051, 'grad_norm': 0.622846245765686, 'learning_rate': 9.7024e-06, 'epoch': 0.8064}
{'loss': 1.105, 'grad_norm': 0.5803285837173462, 'learning_rate': 9.0624e-06, 'epoch': 0.8192}
{'loss': 1.1039, 'grad_norm': 0.5982745885848999, 'learning_rate': 8.4224e-06, 'epoch': 0.832}
{'loss': 1.1038, 'grad_norm': 0.6136074662208557, 'learning_rate': 7.7824e-06, 'epoch': 0.8448}
{'loss': 1.1047, 'grad_norm': 0.5602678060531616, 'learning_rate': 7.1424000000000004e-06, 'epoch': 0.8576}
{'loss': 1.1045, 'grad_norm': 0.5975953936576843, 'learning_rate': 6.5024e-06, 'epoch': 0.8704}
{'loss': 1.1042, 'grad_norm': 0.5917600393295288, 'learning_rate': 5.862400000000001e-06, 'epoch': 0.8832}
{'loss': 1.1039, 'grad_norm': 0.568644642829895, 'learning_rate': 5.2224e-06, 'epoch': 0.896}
{'loss': 1.1044, 'grad_norm': 0.6784006953239441, 'learning_rate': 4.5823999999999995e-06, 'epoch': 0.9088}
{'loss': 1.1047, 'grad_norm': 0.6173162460327148, 'learning_rate': 3.9424000000000006e-06, 'epoch': 0.9216}
{'loss': 1.1042, 'grad_norm': 0.655148983001709, 'learning_rate': 3.3024e-06, 'epoch': 0.9344}
{'loss': 1.1048, 'grad_norm': 0.557979941368103, 'learning_rate': 2.6624e-06, 'epoch': 0.9472}
{'loss': 1.1042, 'grad_norm': 0.7068315744400024, 'learning_rate': 2.0224e-06, 'epoch': 0.96}
{'loss': 1.1035, 'grad_norm': 0.5912676453590393, 'learning_rate': 1.3824e-06, 'epoch': 0.9728}
{'loss': 1.1034, 'grad_norm': 0.5616074800491333, 'learning_rate': 7.424000000000001e-07, 'epoch': 0.9856}
{'loss': 1.1033, 'grad_norm': 0.5618680715560913, 'learning_rate': 1.024e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2316.205, 'train_samples_per_second': 431.741, 'train_steps_per_second': 6.746, 'train_loss': 1.1080733798828124, 'epoch': 1.0}
tokenizer config file saved in ./outputs/ch/hfl_chinese-macbert-base/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/ch/hfl_chinese-macbert-base/unsup/checkpoints/special_tokens_map.json

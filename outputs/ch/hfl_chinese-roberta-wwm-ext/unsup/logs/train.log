/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 102,267,648
{'loss': 1.1368, 'grad_norm': 1.5490282773971558, 'learning_rate': 4.93696e-05, 'epoch': 0.0128}
{'loss': 1.1222, 'grad_norm': 1.26581871509552, 'learning_rate': 4.87296e-05, 'epoch': 0.0256}
{'loss': 1.1198, 'grad_norm': 1.2260571718215942, 'learning_rate': 4.80896e-05, 'epoch': 0.0384}
{'loss': 1.1189, 'grad_norm': 1.147371530532837, 'learning_rate': 4.74496e-05, 'epoch': 0.0512}
{'loss': 1.1166, 'grad_norm': 1.161803960800171, 'learning_rate': 4.680960000000001e-05, 'epoch': 0.064}
{'loss': 1.1158, 'grad_norm': 1.0206329822540283, 'learning_rate': 4.61696e-05, 'epoch': 0.0768}
{'loss': 1.1137, 'grad_norm': 1.3715335130691528, 'learning_rate': 4.55296e-05, 'epoch': 0.0896}
{'loss': 1.1133, 'grad_norm': 1.0171663761138916, 'learning_rate': 4.4889600000000005e-05, 'epoch': 0.1024}
{'loss': 1.1115, 'grad_norm': 0.9783748984336853, 'learning_rate': 4.4249600000000004e-05, 'epoch': 0.1152}
{'loss': 1.1114, 'grad_norm': 0.9722925424575806, 'learning_rate': 4.3609600000000003e-05, 'epoch': 0.128}
{'loss': 1.1105, 'grad_norm': 0.9545606970787048, 'learning_rate': 4.29696e-05, 'epoch': 0.1408}
{'loss': 1.11, 'grad_norm': 1.3374056816101074, 'learning_rate': 4.23296e-05, 'epoch': 0.1536}
{'loss': 1.1101, 'grad_norm': 0.9162869453430176, 'learning_rate': 4.16896e-05, 'epoch': 0.1664}
{'loss': 1.1095, 'grad_norm': 0.9261876940727234, 'learning_rate': 4.104960000000001e-05, 'epoch': 0.1792}
{'loss': 1.109, 'grad_norm': 1.4275448322296143, 'learning_rate': 4.04096e-05, 'epoch': 0.192}
{'loss': 1.1091, 'grad_norm': 0.9476106762886047, 'learning_rate': 3.97696e-05, 'epoch': 0.2048}
{'loss': 1.109, 'grad_norm': 2.57716965675354, 'learning_rate': 3.91296e-05, 'epoch': 0.2176}
{'loss': 1.1085, 'grad_norm': 1.256523609161377, 'learning_rate': 3.8489600000000005e-05, 'epoch': 0.2304}
{'loss': 1.1091, 'grad_norm': 0.8780733346939087, 'learning_rate': 3.7849600000000004e-05, 'epoch': 0.2432}
{'loss': 1.1079, 'grad_norm': 0.8086137175559998, 'learning_rate': 3.72096e-05, 'epoch': 0.256}
{'loss': 1.1076, 'grad_norm': 0.8779845237731934, 'learning_rate': 3.65696e-05, 'epoch': 0.2688}
{'loss': 1.1085, 'grad_norm': 0.7932426929473877, 'learning_rate': 3.59296e-05, 'epoch': 0.2816}
{'loss': 1.1075, 'grad_norm': 0.772845447063446, 'learning_rate': 3.52896e-05, 'epoch': 0.2944}
{'loss': 1.1083, 'grad_norm': 0.7587814927101135, 'learning_rate': 3.46496e-05, 'epoch': 0.3072}
{'loss': 1.1086, 'grad_norm': 0.7907742857933044, 'learning_rate': 3.40096e-05, 'epoch': 0.32}
{'loss': 1.1082, 'grad_norm': 0.7414497137069702, 'learning_rate': 3.33696e-05, 'epoch': 0.3328}
{'loss': 1.1069, 'grad_norm': 0.7380189895629883, 'learning_rate': 3.27296e-05, 'epoch': 0.3456}
{'loss': 1.1063, 'grad_norm': 0.75564044713974, 'learning_rate': 3.2089600000000005e-05, 'epoch': 0.3584}
{'loss': 1.1075, 'grad_norm': 0.7439521551132202, 'learning_rate': 3.1449600000000005e-05, 'epoch': 0.3712}
{'loss': 1.1072, 'grad_norm': 0.7261571288108826, 'learning_rate': 3.08096e-05, 'epoch': 0.384}
{'loss': 1.1068, 'grad_norm': 0.755184531211853, 'learning_rate': 3.0169600000000003e-05, 'epoch': 0.3968}
{'loss': 1.1079, 'grad_norm': 0.7240455150604248, 'learning_rate': 2.9529600000000003e-05, 'epoch': 0.4096}
{'loss': 1.1065, 'grad_norm': 0.6832746863365173, 'learning_rate': 2.88896e-05, 'epoch': 0.4224}
{'loss': 1.1066, 'grad_norm': 0.6705924272537231, 'learning_rate': 2.8249600000000005e-05, 'epoch': 0.4352}
{'loss': 1.106, 'grad_norm': 0.686284601688385, 'learning_rate': 2.76096e-05, 'epoch': 0.448}
{'loss': 1.1057, 'grad_norm': 0.7087846994400024, 'learning_rate': 2.69696e-05, 'epoch': 0.4608}
{'loss': 1.1068, 'grad_norm': 0.6548706889152527, 'learning_rate': 2.63296e-05, 'epoch': 0.4736}
{'loss': 1.106, 'grad_norm': 0.7231473922729492, 'learning_rate': 2.5689600000000002e-05, 'epoch': 0.4864}
{'loss': 1.106, 'grad_norm': 0.7046510577201843, 'learning_rate': 2.50496e-05, 'epoch': 0.4992}
{'loss': 1.1064, 'grad_norm': 0.6678570508956909, 'learning_rate': 2.44096e-05, 'epoch': 0.512}
{'loss': 1.1061, 'grad_norm': 0.7002342343330383, 'learning_rate': 2.37696e-05, 'epoch': 0.5248}
{'loss': 1.1061, 'grad_norm': 0.6868888735771179, 'learning_rate': 2.31296e-05, 'epoch': 0.5376}
{'loss': 1.1062, 'grad_norm': 0.6177839636802673, 'learning_rate': 2.2489600000000002e-05, 'epoch': 0.5504}
{'loss': 1.1051, 'grad_norm': 0.66102135181427, 'learning_rate': 2.18496e-05, 'epoch': 0.5632}
{'loss': 1.1054, 'grad_norm': 0.7294061183929443, 'learning_rate': 2.12096e-05, 'epoch': 0.576}
{'loss': 1.1064, 'grad_norm': 0.6587910652160645, 'learning_rate': 2.05696e-05, 'epoch': 0.5888}
{'loss': 1.1054, 'grad_norm': 0.6492525339126587, 'learning_rate': 1.99296e-05, 'epoch': 0.6016}
{'loss': 1.1055, 'grad_norm': 0.6808009743690491, 'learning_rate': 1.9289600000000002e-05, 'epoch': 0.6144}
{'loss': 1.1058, 'grad_norm': 0.6644516587257385, 'learning_rate': 1.8649600000000002e-05, 'epoch': 0.6272}
{'loss': 1.1047, 'grad_norm': 0.6762024164199829, 'learning_rate': 1.80096e-05, 'epoch': 0.64}
{'loss': 1.1053, 'grad_norm': 0.6347206234931946, 'learning_rate': 1.73696e-05, 'epoch': 0.6528}
{'loss': 1.1049, 'grad_norm': 0.6725571751594543, 'learning_rate': 1.67296e-05, 'epoch': 0.6656}
{'loss': 1.105, 'grad_norm': 0.6375404596328735, 'learning_rate': 1.6089600000000003e-05, 'epoch': 0.6784}
{'loss': 1.1051, 'grad_norm': 0.6164920926094055, 'learning_rate': 1.54496e-05, 'epoch': 0.6912}
{'loss': 1.1042, 'grad_norm': 0.6351601481437683, 'learning_rate': 1.4809600000000001e-05, 'epoch': 0.704}
{'loss': 1.1055, 'grad_norm': 0.5965820550918579, 'learning_rate': 1.4169599999999999e-05, 'epoch': 0.7168}
{'loss': 1.1046, 'grad_norm': 0.6245169043540955, 'learning_rate': 1.35296e-05, 'epoch': 0.7296}
{'loss': 1.1048, 'grad_norm': 0.6213414669036865, 'learning_rate': 1.2889600000000001e-05, 'epoch': 0.7424}
{'loss': 1.1042, 'grad_norm': 0.6241645216941833, 'learning_rate': 1.22496e-05, 'epoch': 0.7552}
{'loss': 1.1044, 'grad_norm': 0.6178849339485168, 'learning_rate': 1.1609600000000001e-05, 'epoch': 0.768}
{'loss': 1.1041, 'grad_norm': 0.6134058237075806, 'learning_rate': 1.0972800000000001e-05, 'epoch': 0.7808}
{'loss': 1.1047, 'grad_norm': 0.6135953664779663, 'learning_rate': 1.03328e-05, 'epoch': 0.7936}
{'loss': 1.1045, 'grad_norm': 0.6282740831375122, 'learning_rate': 9.6928e-06, 'epoch': 0.8064}
{'loss': 1.105, 'grad_norm': 0.6174291372299194, 'learning_rate': 9.056000000000001e-06, 'epoch': 0.8192}
{'loss': 1.104, 'grad_norm': 0.605919599533081, 'learning_rate': 8.416e-06, 'epoch': 0.832}
{'loss': 1.1044, 'grad_norm': 0.5991623997688293, 'learning_rate': 7.776e-06, 'epoch': 0.8448}
{'loss': 1.1047, 'grad_norm': 0.5853544473648071, 'learning_rate': 7.136000000000001e-06, 'epoch': 0.8576}
{'loss': 1.1042, 'grad_norm': 0.5843633413314819, 'learning_rate': 6.496000000000001e-06, 'epoch': 0.8704}
{'loss': 1.1042, 'grad_norm': 0.5955243110656738, 'learning_rate': 5.856e-06, 'epoch': 0.8832}
{'loss': 1.1038, 'grad_norm': 0.5808371901512146, 'learning_rate': 5.219200000000001e-06, 'epoch': 0.896}
{'loss': 1.1037, 'grad_norm': 0.5931118726730347, 'learning_rate': 4.5792e-06, 'epoch': 0.9088}
{'loss': 1.1044, 'grad_norm': 0.6128825545310974, 'learning_rate': 3.9424000000000006e-06, 'epoch': 0.9216}
{'loss': 1.1036, 'grad_norm': 0.6377902626991272, 'learning_rate': 3.3024e-06, 'epoch': 0.9344}
{'loss': 1.1043, 'grad_norm': 0.5946261882781982, 'learning_rate': 2.6624e-06, 'epoch': 0.9472}
{'loss': 1.104, 'grad_norm': 0.6281865835189819, 'learning_rate': 2.0224e-06, 'epoch': 0.96}
{'loss': 1.1039, 'grad_norm': 0.6103541851043701, 'learning_rate': 1.3824e-06, 'epoch': 0.9728}
{'loss': 1.1034, 'grad_norm': 1.0216419696807861, 'learning_rate': 7.424000000000001e-07, 'epoch': 0.9856}
{'loss': 1.1037, 'grad_norm': 1.0419038534164429, 'learning_rate': 1.024e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2320.3929, 'train_samples_per_second': 430.961, 'train_steps_per_second': 6.734, 'train_loss': 1.107600365234375, 'epoch': 1.0}
tokenizer config file saved in ./outputs/ch/hfl_chinese-roberta-wwm-ext/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/ch/hfl_chinese-roberta-wwm-ext/unsup/checkpoints/special_tokens_map.json

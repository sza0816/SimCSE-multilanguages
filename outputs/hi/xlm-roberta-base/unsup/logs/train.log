/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 278,043,648
{'loss': 5.4388, 'grad_norm': 49.55796432495117, 'learning_rate': 4.9382400000000003e-05, 'epoch': 0.0128}
{'loss': 1.3442, 'grad_norm': 4.8178253173828125, 'learning_rate': 4.87424e-05, 'epoch': 0.0256}
{'loss': 1.171, 'grad_norm': 4.022034168243408, 'learning_rate': 4.81024e-05, 'epoch': 0.0384}
{'loss': 1.1502, 'grad_norm': 2.954786777496338, 'learning_rate': 4.74624e-05, 'epoch': 0.0512}
{'loss': 1.1376, 'grad_norm': 2.9545960426330566, 'learning_rate': 4.68224e-05, 'epoch': 0.064}
{'loss': 1.1385, 'grad_norm': 2.4320292472839355, 'learning_rate': 4.61824e-05, 'epoch': 0.0768}
{'loss': 1.133, 'grad_norm': 2.551814079284668, 'learning_rate': 4.55424e-05, 'epoch': 0.0896}
{'loss': 1.1293, 'grad_norm': 2.229945421218872, 'learning_rate': 4.49024e-05, 'epoch': 0.1024}
{'loss': 1.1312, 'grad_norm': 2.012455701828003, 'learning_rate': 4.4262400000000005e-05, 'epoch': 0.1152}
{'loss': 1.127, 'grad_norm': 1.9214067459106445, 'learning_rate': 4.3622400000000004e-05, 'epoch': 0.128}
{'loss': 1.126, 'grad_norm': 1.9423904418945312, 'learning_rate': 4.2982400000000004e-05, 'epoch': 0.1408}
{'loss': 1.1323, 'grad_norm': inf, 'learning_rate': 4.23424e-05, 'epoch': 0.1536}
{'loss': 1.2588, 'grad_norm': 2.1593425273895264, 'learning_rate': 4.17056e-05, 'epoch': 0.1664}
{'loss': 1.1293, 'grad_norm': 1.968526840209961, 'learning_rate': 4.10656e-05, 'epoch': 0.1792}
{'loss': 1.1253, 'grad_norm': 1.9692338705062866, 'learning_rate': 4.04256e-05, 'epoch': 0.192}
{'loss': 1.1221, 'grad_norm': 1.8634881973266602, 'learning_rate': 3.97856e-05, 'epoch': 0.2048}
{'loss': 1.1221, 'grad_norm': 1.4987854957580566, 'learning_rate': 3.914560000000001e-05, 'epoch': 0.2176}
{'loss': 1.124, 'grad_norm': 1.7295713424682617, 'learning_rate': 3.85056e-05, 'epoch': 0.2304}
{'loss': 1.13, 'grad_norm': 2.9426586627960205, 'learning_rate': 3.78656e-05, 'epoch': 0.2432}
{'loss': 1.1488, 'grad_norm': 1.6806375980377197, 'learning_rate': 3.72256e-05, 'epoch': 0.256}
{'loss': 1.1217, 'grad_norm': 3.307795524597168, 'learning_rate': 3.6585600000000004e-05, 'epoch': 0.2688}
{'loss': 1.1212, 'grad_norm': 1.6536645889282227, 'learning_rate': 3.5945600000000004e-05, 'epoch': 0.2816}
{'loss': 1.124, 'grad_norm': 2.377572536468506, 'learning_rate': 3.53056e-05, 'epoch': 0.2944}
{'loss': 1.1216, 'grad_norm': 1.376367449760437, 'learning_rate': 3.46656e-05, 'epoch': 0.3072}
{'loss': 1.1208, 'grad_norm': 1.483689785003662, 'learning_rate': 3.40256e-05, 'epoch': 0.32}
{'loss': 1.1165, 'grad_norm': 1.4631869792938232, 'learning_rate': 3.33856e-05, 'epoch': 0.3328}
{'loss': 1.1164, 'grad_norm': 1.2596408128738403, 'learning_rate': 3.274560000000001e-05, 'epoch': 0.3456}
{'loss': 1.1166, 'grad_norm': 1.4584887027740479, 'learning_rate': 3.21056e-05, 'epoch': 0.3584}
{'loss': 1.116, 'grad_norm': 3.1159112453460693, 'learning_rate': 3.14656e-05, 'epoch': 0.3712}
{'loss': 1.116, 'grad_norm': 1.3466441631317139, 'learning_rate': 3.08256e-05, 'epoch': 0.384}
{'loss': 1.1154, 'grad_norm': 1.1876238584518433, 'learning_rate': 3.01856e-05, 'epoch': 0.3968}
{'loss': 1.1138, 'grad_norm': 1.3007676601409912, 'learning_rate': 2.95456e-05, 'epoch': 0.4096}
{'loss': 1.1154, 'grad_norm': 1.2483024597167969, 'learning_rate': 2.89056e-05, 'epoch': 0.4224}
{'loss': 1.1153, 'grad_norm': 1.2393038272857666, 'learning_rate': 2.8265600000000003e-05, 'epoch': 0.4352}
{'loss': 1.1164, 'grad_norm': 1.2372703552246094, 'learning_rate': 2.7625600000000002e-05, 'epoch': 0.448}
{'loss': 1.1131, 'grad_norm': 1.1336222887039185, 'learning_rate': 2.69856e-05, 'epoch': 0.4608}
{'loss': 1.1125, 'grad_norm': 1.1933026313781738, 'learning_rate': 2.6345600000000004e-05, 'epoch': 0.4736}
{'loss': 1.1129, 'grad_norm': 1.1805638074874878, 'learning_rate': 2.5705600000000003e-05, 'epoch': 0.4864}
{'loss': 1.1117, 'grad_norm': 1.1492691040039062, 'learning_rate': 2.50656e-05, 'epoch': 0.4992}
{'loss': 1.1126, 'grad_norm': 1.1664310693740845, 'learning_rate': 2.4425600000000002e-05, 'epoch': 0.512}
{'loss': 1.1135, 'grad_norm': 1.1338971853256226, 'learning_rate': 2.37856e-05, 'epoch': 0.5248}
{'loss': 1.1159, 'grad_norm': 1.2816822528839111, 'learning_rate': 2.31456e-05, 'epoch': 0.5376}
{'loss': 1.1123, 'grad_norm': 1.1431759595870972, 'learning_rate': 2.25056e-05, 'epoch': 0.5504}
{'loss': 1.1112, 'grad_norm': 1.0206032991409302, 'learning_rate': 2.18656e-05, 'epoch': 0.5632}
{'loss': 1.1131, 'grad_norm': 1.1344871520996094, 'learning_rate': 2.1225600000000002e-05, 'epoch': 0.576}
{'loss': 1.1133, 'grad_norm': 1.1147960424423218, 'learning_rate': 2.05856e-05, 'epoch': 0.5888}
{'loss': 1.1121, 'grad_norm': 1.0093605518341064, 'learning_rate': 1.99456e-05, 'epoch': 0.6016}
{'loss': 1.1137, 'grad_norm': 1.56968355178833, 'learning_rate': 1.93056e-05, 'epoch': 0.6144}
{'loss': 1.1109, 'grad_norm': 1.1043440103530884, 'learning_rate': 1.86656e-05, 'epoch': 0.6272}
{'loss': 1.1122, 'grad_norm': 1.9635627269744873, 'learning_rate': 1.8025600000000002e-05, 'epoch': 0.64}
{'loss': 1.1119, 'grad_norm': 0.9987524747848511, 'learning_rate': 1.7385600000000002e-05, 'epoch': 0.6528}
{'loss': 1.1099, 'grad_norm': 0.994924008846283, 'learning_rate': 1.67456e-05, 'epoch': 0.6656}
{'loss': 1.1116, 'grad_norm': 1.026638150215149, 'learning_rate': 1.61056e-05, 'epoch': 0.6784}
{'loss': 1.1104, 'grad_norm': 0.9589903354644775, 'learning_rate': 1.54656e-05, 'epoch': 0.6912}
{'loss': 1.108, 'grad_norm': 0.9650070667266846, 'learning_rate': 1.48256e-05, 'epoch': 0.704}
{'loss': 1.1112, 'grad_norm': 0.9468607902526855, 'learning_rate': 1.4185600000000002e-05, 'epoch': 0.7168}
{'loss': 1.1107, 'grad_norm': 0.9578039050102234, 'learning_rate': 1.35488e-05, 'epoch': 0.7296}
{'loss': 1.112, 'grad_norm': 0.9068461060523987, 'learning_rate': 1.29088e-05, 'epoch': 0.7424}
{'loss': 1.1094, 'grad_norm': 0.8909653425216675, 'learning_rate': 1.2268800000000002e-05, 'epoch': 0.7552}
{'loss': 1.11, 'grad_norm': 6.17611026763916, 'learning_rate': 1.1628800000000001e-05, 'epoch': 0.768}
{'loss': 1.1092, 'grad_norm': 1.384641170501709, 'learning_rate': 1.09888e-05, 'epoch': 0.7808}
{'loss': 1.1121, 'grad_norm': 0.9250192642211914, 'learning_rate': 1.03488e-05, 'epoch': 0.7936}
{'loss': 1.1119, 'grad_norm': 0.9466758370399475, 'learning_rate': 9.7088e-06, 'epoch': 0.8064}
{'loss': 1.1085, 'grad_norm': 0.9182705283164978, 'learning_rate': 9.0688e-06, 'epoch': 0.8192}
{'loss': 1.1095, 'grad_norm': 1.0466008186340332, 'learning_rate': 8.428800000000001e-06, 'epoch': 0.832}
{'loss': 1.1087, 'grad_norm': 1.1849031448364258, 'learning_rate': 7.7888e-06, 'epoch': 0.8448}
{'loss': 1.1092, 'grad_norm': 0.8603275418281555, 'learning_rate': 7.1488e-06, 'epoch': 0.8576}
{'loss': 1.1089, 'grad_norm': 0.9171090722084045, 'learning_rate': 6.508799999999999e-06, 'epoch': 0.8704}
{'loss': 1.1087, 'grad_norm': 0.880771279335022, 'learning_rate': 5.8688e-06, 'epoch': 0.8832}
{'loss': 1.1086, 'grad_norm': 0.8878781795501709, 'learning_rate': 5.228800000000001e-06, 'epoch': 0.896}
{'loss': 1.1088, 'grad_norm': 0.9108250141143799, 'learning_rate': 4.5888e-06, 'epoch': 0.9088}
{'loss': 1.1079, 'grad_norm': 0.9818381667137146, 'learning_rate': 3.9488e-06, 'epoch': 0.9216}
{'loss': 1.1098, 'grad_norm': 0.9203827381134033, 'learning_rate': 3.3088e-06, 'epoch': 0.9344}
{'loss': 1.1095, 'grad_norm': 0.8890911936759949, 'learning_rate': 2.6688e-06, 'epoch': 0.9472}
{'loss': 1.1081, 'grad_norm': 0.9906837344169617, 'learning_rate': 2.0288e-06, 'epoch': 0.96}
{'loss': 1.1112, 'grad_norm': 0.9475804567337036, 'learning_rate': 1.3888e-06, 'epoch': 0.9728}
{'loss': 1.1088, 'grad_norm': 0.8664950132369995, 'learning_rate': 7.488e-07, 'epoch': 0.9856}
{'loss': 1.108, 'grad_norm': 0.8728328347206116, 'learning_rate': 1.0880000000000001e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2729.0531, 'train_samples_per_second': 366.427, 'train_steps_per_second': 5.725, 'train_loss': 1.1774623880615234, 'epoch': 1.0}
tokenizer config file saved in ./outputs/hi/xlm-roberta-base/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/hi/xlm-roberta-base/unsup/checkpoints/special_tokens_map.json

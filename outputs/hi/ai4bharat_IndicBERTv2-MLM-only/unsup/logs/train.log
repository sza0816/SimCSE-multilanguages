/workspace/simcse/SimCSE-multilanguages/train.py:151: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Using auto half precision backend
***** Running training *****
  Num examples = 1,000,000
  Num Epochs = 1
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 15,625
  Number of trainable parameters = 278,041,344
{'loss': 1.2158, 'grad_norm': 0.8751904964447021, 'learning_rate': 4.93696e-05, 'epoch': 0.0128}
{'loss': 1.1178, 'grad_norm': 0.8477248549461365, 'learning_rate': 4.87296e-05, 'epoch': 0.0256}
{'loss': 1.1149, 'grad_norm': 0.7090318202972412, 'learning_rate': 4.80896e-05, 'epoch': 0.0384}
{'loss': 1.1134, 'grad_norm': 0.6667271256446838, 'learning_rate': 4.74496e-05, 'epoch': 0.0512}
{'loss': 1.1123, 'grad_norm': 0.645778477191925, 'learning_rate': 4.680960000000001e-05, 'epoch': 0.064}
{'loss': 1.1132, 'grad_norm': 0.647750735282898, 'learning_rate': 4.61696e-05, 'epoch': 0.0768}
{'loss': 1.1107, 'grad_norm': 0.7020595073699951, 'learning_rate': 4.55296e-05, 'epoch': 0.0896}
{'loss': 1.1117, 'grad_norm': 0.6712732911109924, 'learning_rate': 4.4889600000000005e-05, 'epoch': 0.1024}
{'loss': 1.1134, 'grad_norm': 1.0856956243515015, 'learning_rate': 4.42528e-05, 'epoch': 0.1152}
{'loss': 1.1152, 'grad_norm': 0.969119668006897, 'learning_rate': 4.3619200000000004e-05, 'epoch': 0.128}
{'loss': 1.1135, 'grad_norm': 0.6703799366950989, 'learning_rate': 4.2982400000000004e-05, 'epoch': 0.1408}
{'loss': 1.115, 'grad_norm': 0.9135822653770447, 'learning_rate': 4.23424e-05, 'epoch': 0.1536}
{'loss': 1.1134, 'grad_norm': 0.7535748481750488, 'learning_rate': 4.17056e-05, 'epoch': 0.1664}
{'loss': 1.1102, 'grad_norm': 2.1142847537994385, 'learning_rate': 4.10688e-05, 'epoch': 0.1792}
{'loss': 1.1133, 'grad_norm': 1.010211706161499, 'learning_rate': 4.0432e-05, 'epoch': 0.192}
{'loss': 1.1112, 'grad_norm': 1.568990707397461, 'learning_rate': 3.9792e-05, 'epoch': 0.2048}
{'loss': 1.1135, 'grad_norm': 1.7819699048995972, 'learning_rate': 3.9152e-05, 'epoch': 0.2176}
{'loss': 1.116, 'grad_norm': 0.8259579539299011, 'learning_rate': 3.851200000000001e-05, 'epoch': 0.2304}
{'loss': 1.116, 'grad_norm': 4.300045967102051, 'learning_rate': 3.7872e-05, 'epoch': 0.2432}
{'loss': 1.1137, 'grad_norm': 0.7983722686767578, 'learning_rate': 3.7232e-05, 'epoch': 0.256}
{'loss': 1.115, 'grad_norm': 0.7696144580841064, 'learning_rate': 3.6592000000000005e-05, 'epoch': 0.2688}
{'loss': 1.1138, 'grad_norm': 1.2046124935150146, 'learning_rate': 3.5952000000000004e-05, 'epoch': 0.2816}
{'loss': 1.1121, 'grad_norm': 1.8728879690170288, 'learning_rate': 3.5312000000000003e-05, 'epoch': 0.2944}
{'loss': 1.1134, 'grad_norm': 3.1695563793182373, 'learning_rate': 3.4671999999999996e-05, 'epoch': 0.3072}
{'loss': 1.1133, 'grad_norm': 2.3950157165527344, 'learning_rate': 3.4032e-05, 'epoch': 0.32}
{'loss': 1.11, 'grad_norm': 2.7458057403564453, 'learning_rate': 3.3392e-05, 'epoch': 0.3328}
{'loss': 1.1093, 'grad_norm': 1.879915714263916, 'learning_rate': 3.2752e-05, 'epoch': 0.3456}
{'loss': 1.1101, 'grad_norm': 257.81243896484375, 'learning_rate': 3.2112e-05, 'epoch': 0.3584}
{'loss': 1.11, 'grad_norm': 1.4108564853668213, 'learning_rate': 3.1472e-05, 'epoch': 0.3712}
{'loss': 1.1096, 'grad_norm': 1.012433648109436, 'learning_rate': 3.0832e-05, 'epoch': 0.384}
{'loss': 1.1085, 'grad_norm': 2.2090988159179688, 'learning_rate': 3.0192000000000005e-05, 'epoch': 0.3968}
{'loss': 1.1092, 'grad_norm': 0.6405020356178284, 'learning_rate': 2.9552e-05, 'epoch': 0.4096}
{'loss': 1.1116, 'grad_norm': 1.4333689212799072, 'learning_rate': 2.8912e-05, 'epoch': 0.4224}
{'loss': 1.1115, 'grad_norm': 0.6666821837425232, 'learning_rate': 2.8272000000000003e-05, 'epoch': 0.4352}
{'loss': 1.1121, 'grad_norm': 0.8688942790031433, 'learning_rate': 2.7632000000000002e-05, 'epoch': 0.448}
{'loss': 1.1104, 'grad_norm': 0.6240935921669006, 'learning_rate': 2.6992000000000002e-05, 'epoch': 0.4608}
{'loss': 1.1099, 'grad_norm': 0.6323919296264648, 'learning_rate': 2.6351999999999998e-05, 'epoch': 0.4736}
{'loss': 1.1095, 'grad_norm': 1.1097609996795654, 'learning_rate': 2.5712000000000004e-05, 'epoch': 0.4864}
{'loss': 1.109, 'grad_norm': 1.4169644117355347, 'learning_rate': 2.5072e-05, 'epoch': 0.4992}
{'loss': 1.1087, 'grad_norm': 1.4354780912399292, 'learning_rate': 2.4432000000000003e-05, 'epoch': 0.512}
{'loss': 1.1092, 'grad_norm': 0.7737559676170349, 'learning_rate': 2.3792000000000002e-05, 'epoch': 0.5248}
{'loss': 1.1089, 'grad_norm': 0.610621452331543, 'learning_rate': 2.3152e-05, 'epoch': 0.5376}
{'loss': 1.109, 'grad_norm': 0.6004284024238586, 'learning_rate': 2.25152e-05, 'epoch': 0.5504}
{'loss': 1.1077, 'grad_norm': 0.48350995779037476, 'learning_rate': 2.18752e-05, 'epoch': 0.5632}
{'loss': 1.1111, 'grad_norm': 0.49874427914619446, 'learning_rate': 2.1235200000000003e-05, 'epoch': 0.576}
{'loss': 1.1095, 'grad_norm': 0.5117483735084534, 'learning_rate': 2.05952e-05, 'epoch': 0.5888}
{'loss': 1.1087, 'grad_norm': 0.691749632358551, 'learning_rate': 1.99552e-05, 'epoch': 0.6016}
{'loss': 1.1102, 'grad_norm': 0.6454456448554993, 'learning_rate': 1.93184e-05, 'epoch': 0.6144}
{'loss': 1.1067, 'grad_norm': 0.4866592288017273, 'learning_rate': 1.86784e-05, 'epoch': 0.6272}
{'loss': 1.1083, 'grad_norm': 0.7386250495910645, 'learning_rate': 1.80384e-05, 'epoch': 0.64}
{'loss': 1.1081, 'grad_norm': 0.8421458601951599, 'learning_rate': 1.73984e-05, 'epoch': 0.6528}
{'loss': 1.1067, 'grad_norm': 0.8212217092514038, 'learning_rate': 1.6758400000000002e-05, 'epoch': 0.6656}
{'loss': 1.1082, 'grad_norm': 0.5158920884132385, 'learning_rate': 1.61184e-05, 'epoch': 0.6784}
{'loss': 1.1071, 'grad_norm': 0.46496450901031494, 'learning_rate': 1.54784e-05, 'epoch': 0.6912}
{'loss': 1.1067, 'grad_norm': 0.4710835814476013, 'learning_rate': 1.48384e-05, 'epoch': 0.704}
{'loss': 1.1079, 'grad_norm': 0.45360198616981506, 'learning_rate': 1.4198400000000001e-05, 'epoch': 0.7168}
{'loss': 1.1079, 'grad_norm': 0.4915916621685028, 'learning_rate': 1.3558400000000002e-05, 'epoch': 0.7296}
{'loss': 1.1085, 'grad_norm': 0.4451511800289154, 'learning_rate': 1.29184e-05, 'epoch': 0.7424}
{'loss': 1.1065, 'grad_norm': 0.4953840970993042, 'learning_rate': 1.2278400000000001e-05, 'epoch': 0.7552}
{'loss': 1.108, 'grad_norm': 1.041330099105835, 'learning_rate': 1.16384e-05, 'epoch': 0.768}
{'loss': 1.1071, 'grad_norm': 0.575160562992096, 'learning_rate': 1.09984e-05, 'epoch': 0.7808}
{'loss': 1.1092, 'grad_norm': 0.4874066114425659, 'learning_rate': 1.03584e-05, 'epoch': 0.7936}
{'loss': 1.1097, 'grad_norm': 0.4388827681541443, 'learning_rate': 9.718400000000002e-06, 'epoch': 0.8064}
{'loss': 1.1061, 'grad_norm': 0.4933602511882782, 'learning_rate': 9.078400000000001e-06, 'epoch': 0.8192}
{'loss': 1.107, 'grad_norm': 0.443150132894516, 'learning_rate': 8.4384e-06, 'epoch': 0.832}
{'loss': 1.1061, 'grad_norm': 0.6608211398124695, 'learning_rate': 7.7984e-06, 'epoch': 0.8448}
{'loss': 1.1067, 'grad_norm': 0.6535482406616211, 'learning_rate': 7.1584e-06, 'epoch': 0.8576}
{'loss': 1.1068, 'grad_norm': 0.5632246732711792, 'learning_rate': 6.518400000000001e-06, 'epoch': 0.8704}
{'loss': 1.106, 'grad_norm': 2.1338915824890137, 'learning_rate': 5.8784e-06, 'epoch': 0.8832}
{'loss': 1.1065, 'grad_norm': 0.5114314556121826, 'learning_rate': 5.2384000000000005e-06, 'epoch': 0.896}
{'loss': 1.1064, 'grad_norm': 0.476257860660553, 'learning_rate': 4.5984e-06, 'epoch': 0.9088}
{'loss': 1.1056, 'grad_norm': 0.5423261523246765, 'learning_rate': 3.9584e-06, 'epoch': 0.9216}
{'loss': 1.1077, 'grad_norm': 0.4575499892234802, 'learning_rate': 3.3184e-06, 'epoch': 0.9344}
{'loss': 1.1073, 'grad_norm': 0.5085752606391907, 'learning_rate': 2.6784e-06, 'epoch': 0.9472}
{'loss': 1.1048, 'grad_norm': 0.5970478653907776, 'learning_rate': 2.0384e-06, 'epoch': 0.96}
{'loss': 1.1089, 'grad_norm': 0.48350077867507935, 'learning_rate': 1.3984e-06, 'epoch': 0.9728}
{'loss': 1.1073, 'grad_norm': 0.5369258522987366, 'learning_rate': 7.616e-07, 'epoch': 0.9856}
{'loss': 1.1059, 'grad_norm': 0.4689953327178955, 'learning_rate': 1.216e-07, 'epoch': 0.9984}


Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2617.6629, 'train_samples_per_second': 382.02, 'train_steps_per_second': 5.969, 'train_loss': 1.111303569946289, 'epoch': 1.0}
tokenizer config file saved in ./outputs/hi/ai4bharat_IndicBERTv2-MLM-only/unsup/checkpoints/tokenizer_config.json
Special tokens file saved in ./outputs/hi/ai4bharat_IndicBERTv2-MLM-only/unsup/checkpoints/special_tokens_map.json
